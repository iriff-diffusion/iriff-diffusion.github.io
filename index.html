<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Improving Semantic Perception with Rectified Flow Features">
  <meta property="og:title" content="Improving Semantic Perception with Rectified Flow Features"/>
  <meta property="og:description" content="We present LoRACLR, a method for merging personalized LoRA models for multi-concept image generation, aligning model weights with a contrastive objective to ensure distinct, high-quality concept synthesis."/>
  <meta property="og:url" content="https://enis.dev/loraclr"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/teaser.jpg" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Multi-concept image generation, LoRA, contrastive objective, lora composition, personalized image synthesis">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <script src="https://www.w3counter.com/tracker.js?id=155988"></script>

  <title>Improving Semantic Perception with Rectified Flow Features</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Improving Semantic Perception with Rectified Flow Features</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://annusha.github.io/" target="_blank">Anna Kukleva</a><sup>1,&#8224;</sup>,</span>
              <span class="author-block">
                <a href="https://enisimsar.github.io/" target="_blank">Enis Simsar</a><sup>2,&#8224;</sup>,</span>
              <span class="author-block">
                <a href="https://alessiotonioni.github.io/" target="_blank">Alessio Tonioni</a><sup>4</sup>,</span>
              <span class="author-block">
                <a href="https://ferjad.github.io/" target="_blank">Muhammad Ferjad Naeem</a><sup>4</sup>,</span>
              <br>
              <span class="author-block">
                <a href="https://da.inf.ethz.ch/" target="_blank">Thomas Hofmann</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://federicotombari.github.io/" target="_blank">Federico Tombari</a><sup>3,4</sup>,</span>
              <span class="author-block">
                <a href="https://janericlenssen.github.io/" target="_blank">Jan Eric Lenssen</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele" target="_blank">Bernt Schiele</a><sup>1</sup>,</span>
              </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Max Planck Institute for Informatics, SIC,</span>
                    <span class="author-block"><sup>2</sup>ETH ZÃ¼rich - DALAB,</span>
                    <span class="author-block"><sup>3</sup>TU Munich,</span>
                    <span class="author-block"><sup>4</sup>Google</span>
                    <span class="eql-cntrb" style="font-size: 90%; display: block; margin-top: 4px;">
                      <sup>&#8224;</sup>Indicates Equal Contribution
                    </span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <!-- ArXiv abstract Link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/<ARXIV_ID>" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                      </a>
                    </span>
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV_ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                <span class="link-block">
                    <a href="https://github.com/CODE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We propose RIFF and iRIFF, reliable and generalizable methods to extract semantic features from rectified flow models that significantly improve downstream task performance compared to existing semantic feature extractors. Features from large-scale image generative models are known to encode rich semantic information as recently demonstrated by many methods leveraging diffusion models as general feature extractors. However, existing methods have several limitations that we wish to overcome: they often require fine-tuning or combining multiple pre-trained models to achieve better performance. Instead, our approach is the first to extract and analyze features from rectified flow models, which leads to significantly improved downstream quality without additional bells and whistles. We employ a flow inversion mechanism to improve feature quality further and enhance the robustness of feature extraction by aligning the input noise with the data. In addition to achieving state-of-the-art results in zero-shot semantic correspondence, we extend the established set of feature benchmarks by vision-language grounding tasks for both images and videos and propose a novel grounding technique, purely based on cross-attention and without requiring changes to the existing models. We show that stop words can be used to attract and filter out attention pollution. Our results show that rectified flow features significantly outperform previous works for zero-shot grounding without introducing additional fine-tuning or components.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Introduction and Problem Statement -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <h2 class="title">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            <strong>The Challenge:</strong> While diffusion models have revolutionized image generation, extracting meaningful semantic features from these models remains challenging. Existing methods typically rely on older diffusion architectures like Stable Diffusion 2.1, require extensive fine-tuning, or need to combine multiple large pre-trained models to achieve good performance.
          </p>
          <p>
            <strong>The Opportunity:</strong> State-of-the-art image generators are shifting from diffusion models to <em>rectified flow models</em>, which offer more efficient and direct sampling through deterministic integration. However, no prior work has successfully extracted and analyzed semantic features from these newer architectures.
          </p>
          <p>
            <strong>Our Solution:</strong> We introduce the first methods to extract high-quality semantic features from rectified flow models, specifically targeting DiT (Diffusion Transformer) architectures used in modern generators like FLUX and Mochi. Our approach is embarrassingly simple yet highly effective, achieving significant improvements across multiple benchmarks without requiring fine-tuning or model combinations.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Introduction -->

<!-- Key Contributions -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <h2 class="title">Key Contributions</h2>
        <div class="contributions-grid">
          <div class="contribution-card">
            <h4><strong>ðŸš€ First Inverted Rectified Flow Features</strong></h4>
            <p>We are the first to successfully extract semantic features from rectified flow models, unlocking the potential of modern generative architectures for computer vision tasks.</p>
          </div>
          
          <div class="contribution-card">
            <h4><strong>ðŸ”„ Flow Inversion for Robustness</strong></h4>
            <p>Our iRIFF variant uses flow inversion to obtain structured latents that align with the data distribution, significantly improving feature quality and robustness.</p>
          </div>
          
          <div class="contribution-card">
            <h4><strong>ðŸŽ¯ Stop-word Attention Filtering</strong></h4>
            <p>We discover and exploit how stop words "pollute" cross-attention maps, using them as attention magnets to improve vision-language grounding.</p>
          </div>
          
          <div class="contribution-card">
            <h4><strong>ðŸ“Š Zero-shot Multi-domain Results</strong></h4>
            <p>We extend semantic feature evaluation to both images and videos, achieving state-of-the-art results in zero-shot settings across multiple benchmarks.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Key Contributions -->

<!-- Method Overview -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h2 class="title">Method Overview</h2>
        <div class="figure-container">
          <img src="static/images/RIFF_method.jpg" alt="RIFF Method Overview">
          <div class="figure-caption">
            <strong>Extracting DiT Features.</strong> We introduce RIFF and iRIFF, methods for extracting semantic features from rectified flow models using DiT architectures. RIFF injects scaled noise into clean latents, while iRIFF leverages flow inversion to obtain structured latents aligned with the data distribution. Features are extracted from intermediate DiT attention blocks, outperforming traditional U-Net-based features. The diagram shows double- and single-stream transformer blocks, highlighting semantic feature extraction points. V-L denotes vision-language tasks and V denotes vision tasks.
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Method Overview -->

<!-- Stop-word Filtering Results -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h2 class="title">Stop-word Filtering for Referral Segmentation</h2>
        
        <div class="content has-text-justified">
          <p>
            A key discovery in our work is that stop words (e.g., "the", "a", "of") act as attention magnets in cross-attention maps, absorbing significant attention scores and creating noisy backgrounds that hurt segmentation quality. We exploit this phenomenon by strategically adding extra stop words to referral expressions, which further concentrate the attention pollution, and then filtering out all stop words to obtain cleaner attention maps. This simple yet effective technique dramatically improves the quality of attention-based segmentation across both image and video domains, leading to more precise object localization.
          </p>
        </div>

        <div class="figure-container">
          <img src="static/images/cows.jpg" alt="Stop-word Filtering Example">
          <div class="figure-caption">
            <strong>Influence of stop words on referral segmentation.</strong> We demonstrate how stop words "pollute" cross-attention scores by attracting high attention to background areas. By adding extra stop words as attention magnets and then filtering them out, we achieve sharper attention maps focused on core concepts (nouns, verbs, adjectives). The example shows attention maps before and after stop-word filtering, with segmentation results using SAM 2. Gray tokens indicate filtered stop words.
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Stop-word Filtering Results -->

<!-- Semantic Correspondence Results -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h2 class="title">Semantic Correspondence Results</h2>

        <div class="content has-text-justified">
          <p>
            On semantic correspondence benchmarks (SPair-71k, PF-Pascal), our methods establish new state-of-the-art results. iRIFF consistently outperforms RIFF, demonstrating the importance of proper latent alignment through flow inversion. Compared to previous single-model approaches like DIFT, we achieve substantial improvements while using a much simpler pipeline - no fine-tuning, no model combinations, just better base models and smarter feature extraction.
          </p>
        </div>

        <div class="figure-container">
          <img src="static/images/semantic_qual.jpg" alt="Semantic Correspondence Examples">
          <div class="figure-caption">
            <strong>Semantic Correspondence Examples.</strong> Our RIFF and iRIFF methods achieve state-of-the-art performance in finding semantic correspondences between images. The examples show how our rectified flow features successfully identify semantically corresponding points across different objects of the same category, despite variations in appearance, pose, and viewpoint. Our approach achieves a 12.8% performance gain compared to previous single-model features.
          </div>
        </div>

        <div class="table-container">
          <h3><strong>SPair-71k Semantic Correspondence Results (PCK@0.1)</strong></h3>
          <div style="overflow-x: auto;">
            <table class="table is-bordered is-striped is-narrow">
              <thead>
                <tr>
                  <th>Method</th>
                  <th>Plane</th>
                  <th>Bicycle</th>
                  <th>Bird</th>
                  <th>Boat</th>
                  <th>Bottle</th>
                  <th>Bus</th>
                  <th>Car</th>
                  <th>Cat</th>
                  <th>Chair</th>
                  <th>Cow</th>
                  <th>Dog</th>
                  <th>Horse</th>
                  <th>Motorbike</th>
                  <th>Person</th>
                  <th>Plant</th>
                  <th>Sheep</th>
                  <th>Train</th>
                  <th>TV</th>
                  <th><strong>Average</strong></th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>DINOv2</td>
                  <td>53.5</td><td>54.0</td><td>60.2</td><td>35.5</td><td>44.4</td><td>36.3</td><td>31.7</td><td>61.3</td><td>37.4</td><td>54.7</td><td>52.5</td><td>51.5</td><td>48.8</td><td>48.2</td><td>37.8</td><td>44.1</td><td>47.4</td><td>38.2</td><td>46.5</td>
                </tr>
                <tr>
                  <td>DIFT</td>
                  <td>63.5</td><td>54.5</td><td>80.8</td><td>34.5</td><td>46.2</td><td>52.7</td><td>48.3</td><td>77.7</td><td>39.0</td><td>76.0</td><td>54.9</td><td>61.3</td><td>53.3</td><td>46.0</td><td><u>57.8</u></td><td>57.1</td><td>71.1</td><td><strong>63.4</strong></td><td>57.7</td>
                </tr>
                <tr>
                  <td>SD + DINOv2</td>
                  <td><u>73.0</u></td><td><strong>64.1</strong></td><td><strong>86.4</strong></td><td>40.7</td><td><u>52.9</u></td><td>55.0</td><td>53.8</td><td>78.6</td><td>45.5</td><td>77.3</td><td><u>64.7</u></td><td><u>69.7</u></td><td>63.3</td><td><strong>69.2</strong></td><td><strong>58.4</strong></td><td><strong>67.6</strong></td><td>66.2</td><td>53.5</td><td><u>64.0</u></td>
                </tr>
                <tr style="background-color: #f0f8ff;">
                  <td><strong>RIFF (ours)</strong></td>
                  <td>72.6</td><td>62.8</td><td>80.1</td><td><u>44.7</u></td><td>50.0</td><td><u>64.8</u></td><td><strong>56.1</strong></td><td><u>82.8</u></td><td><u>45.7</u></td><td><u>79.6</u></td><td><strong>65.6</strong></td><td>67.2</td><td><u>65.9</u></td><td>64.0</td><td>57.0</td><td>58.0</td><td><strong>70.5</strong></td><td><u>61.6</u></td><td>63.9</td>
                </tr>
                <tr style="background-color: #f0f8ff;">
                  <td><strong>iRIFF (ours)</strong></td>
                  <td><strong>73.8</strong></td><td><u>63.5</u></td><td><u>84.2</u></td><td><strong>45.0</strong></td><td><strong>53.2</strong></td><td><strong>66.2</strong></td><td><u>55.8</u></td><td><strong>83.6</strong></td><td><strong>46.7</strong></td><td><strong>81.0</strong></td><td>64.1</td><td><strong>70.7</strong></td><td><strong>69.2</strong></td><td><u>69.0</u></td><td>55.5</td><td><u>61.0</u></td><td><u>68.1</u></td><td>60.7</td><td><strong>65.1</strong></td>
                </tr>
              </tbody>
            </table>
          </div>
          <p><small><strong>Bold</strong> = best, <u>underlined</u> = second-best. Our single model outperforms the combination of SD + DINOv2.</small></p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Semantic Correspondence Results -->

<!-- Detailed Results Discussion -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <h2 class="title">Referral Image Object Segmentation</h2>

        <div class="content has-text-justified">
          <p>
            We extend semantic feature evaluation beyond pure vision tasks to vision-language grounding, testing on RefCOCO/RefCOCO+/RefCOCOg datasets. Our stop-word filtering technique proves crucial - without it, attention maps are dominated by background noise. With filtering, we achieve remarkable zero-shot performance that rivals specialized grounding models, but with much simpler architecture requirements. Our approach achieves 16.4% performance gain over previous training-free methods.
          </p>
        </div>

        <div class="figure-container">
          <img src="static/images/rios_qual.jpg" alt="Referral Image Object Segmentation Examples">
          <div class="figure-caption">
            <strong>Referral Image Object Segmentation Examples.</strong> Our method leverages cross-attention maps from rectified flow models to achieve zero-shot referral segmentation. By applying stop-word filtering to attention maps and using SAM for mask generation, we achieve state-of-the-art results on RefCOCO benchmarks without requiring any task-specific training or fine-tuning.
          </div>
        </div>
          
        <div class="table-container">
          <h4><strong>RefCOCO Image Referral Segmentation Results</strong></h4>
          <div style="overflow-x: auto;">
            <table class="table is-bordered is-striped is-narrow">
              <thead>
                <tr>
                  <th rowspan="2">Method</th>
                  <th rowspan="2">Vision Backbone</th>
                  <th colspan="3">RefCOCO (oIoU)</th>
                  <th colspan="3">RefCOCO+ (oIoU)</th>
                  <th colspan="2">RefCOCOg (oIoU)</th>
                </tr>
                <tr>
                  <th>val</th>
                  <th>testA</th>
                  <th>testB</th>
                  <th>val</th>
                  <th>testA</th>
                  <th>testB</th>
                  <th>val</th>
                  <th>test</th>
                </tr>
              </thead>
              <tbody>
                <tr style="background-color: #f9f9f9;">
                  <td colspan="10"><em>Zero-shot methods w/o additional training</em></td>
                </tr>
                <tr>
                  <td>Grad-CAM</td>
                  <td>R50</td>
                  <td>23.44</td><td>23.91</td><td>21.60</td><td>26.67</td><td>27.20</td><td>24.84</td><td>23.00</td><td>23.91</td>
                </tr>
                <tr>
                  <td>Global-Local</td>
                  <td>R50</td>
                  <td>24.55</td><td>26.00</td><td>21.03</td><td>26.62</td><td>29.99</td><td>22.23</td><td>28.92</td><td>30.48</td>
                </tr>
                <tr>
                  <td>Global-Local</td>
                  <td>ViT-B</td>
                  <td>21.71</td><td>24.48</td><td>20.51</td><td>23.70</td><td>28.12</td><td>21.86</td><td>26.57</td><td>28.21</td>
                </tr>
                <tr>
                  <td>Ref-Diff</td>
                  <td>ViT-B</td>
                  <td>35.16</td><td>37.44</td><td><u>34.50</u></td><td>35.56</td><td>38.66</td><td>31.40</td><td>38.62</td><td>37.50</td>
                </tr>
                <tr>
                  <td>TAS</td>
                  <td>ViT-B</td>
                  <td>29.53</td><td>30.26</td><td>28.24</td><td>33.21</td><td>38.77</td><td>28.01</td><td>35.84</td><td>36.16</td>
                </tr>
                <tr style="background-color: #f0f8ff;">
                  <td><strong>RIFF (ours)</strong></td>
                  <td>DiT</td>
                  <td><u>38.29</u></td><td><u>43.07</u></td><td>34.01</td><td><u>39.58</u></td><td><u>44.78</u></td><td><u>35.01</u></td><td><u>39.45</u></td><td><u>39.53</u></td>
                </tr>
                <tr style="background-color: #f0f8ff;">
                  <td><strong>iRIFF (ours)</strong></td>
                  <td>DiT</td>
                  <td><strong>39.23</strong></td><td><strong>44.05</strong></td><td><strong>35.34</strong></td><td><strong>41.71</strong></td><td><strong>45.24</strong></td><td><strong>35.95</strong></td><td><strong>40.25</strong></td><td><strong>40.38</strong></td>
                </tr>
              </tbody>
            </table>
          </div>
          <p><small><strong>Bold</strong> = best, <u>underlined</u> = second-best among training-free methods.</small></p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Detailed Results Discussion -->

<!-- Video Referral Segmentation -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h2 class="title">Video Referral Object Segmentation</h2>

        <div class="content has-text-justified">
          <p>
            We demonstrate that our rectified flow features scale effectively to video understanding tasks. Using Mochi (a video rectified flow model), we extract features from the first frame and leverage SAM2's temporal propagation capabilities for consistent video segmentation. Our stop-word filtering technique proves even more crucial in video contexts where temporal consistency amplifies attention noise. The results show substantial improvements over existing training-free methods, establishing new benchmarks for zero-shot video referral segmentation.
          </p>
        </div>

        <div class="figure-container">
          <img src="static/images/davis.jpg" alt="Video Referral Segmentation Results">
          <div class="figure-caption">
            <strong>Video Referral Object Segmentation Examples.</strong> Our method extends seamlessly to video domains using Mochi rectified flow models. We extract cross-attention maps from the first frame and use SAM2 for temporal propagation. The approach is training-free and operates in zero-shot manner, achieving an 18% performance gain over previous methods on video referral segmentation benchmarks.
          </div>
        </div>
        
        <br>
        <div class="columns">
          <div class="column">
            <div class="table-container">
              <h4><strong>Ref-DAVIS17 Video Results</strong></h4>
              <table class="table is-bordered is-striped is-narrow">
                <thead>
                  <tr>
                    <th>Method</th>
                    <th>J&F</th>
                    <th>J</th>
                    <th>F</th>
                  </tr>
                </thead>
                <tbody>
                  <tr style="background-color: #f9f9f9;">
                    <td colspan="4"><em>Training-Free with Grounded-SAM</em></td>
                  </tr>
                  <tr>
                    <td>Grounded-SAM</td>
                    <td>65.2</td><td>62.3</td><td>68.0</td>
                  </tr>
                  <tr>
                    <td>Grounded-SAM2</td>
                    <td>66.2</td><td>62.6</td><td>69.7</td>
                  </tr>
                  <tr>
                    <td>AL-Ref-SAM2</td>
                    <td>74.2</td><td>70.4</td><td>78.0</td>
                  </tr>
                  <tr style="background-color: #f9f9f9;">
                    <td colspan="4"><em>Training-Free</em></td>
                  </tr>
                  <tr>
                    <td>G-L + SAM2</td>
                    <td>40.6</td><td>37.6</td><td>43.6</td>
                  </tr>
                  <tr>
                    <td>G-L (SAM) + SAM2</td>
                    <td>46.9</td><td>44.0</td><td>49.7</td>
                  </tr>
                  <tr style="background-color: #f0f8ff;">
                    <td><strong>RIFF + SAM2 (ours)</strong></td>
                    <td><u>53.7</u></td><td><strong>51.1</strong></td><td><u>56.3</u></td>
                  </tr>
                  <tr style="background-color: #f0f8ff;">
                    <td><strong>iRIFF + SAM2 (ours)</strong></td>
                    <td><strong>54.6</strong></td><td><u>50.9</u></td><td><strong>58.2</strong></td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>
          <div class="column">
            <div class="table-container">
              <h4><strong>Component Ablation Study</strong></h4>
              <table class="table is-bordered is-striped is-narrow">
                <thead>
                  <tr>
                    <th>Inv.</th>
                    <th>E-SW</th>
                    <th>SW</th>
                    <th>SAM2</th>
                    <th>J&F</th>
                    <th>J</th>
                    <th>F</th>
                    <th>PA</th>
                  </tr>
                </thead>
                <tbody>
                  <tr style="background-color: #f0f8ff;">
                    <td>âœ“</td><td>âœ“</td><td>âœ“</td><td>H</td><td><strong>54.6</strong></td><td>50.9</td><td><strong>58.2</strong></td><td><strong>60.2</strong></td>
                  </tr>
                  <tr>
                    <td>-</td><td>âœ“</td><td>âœ“</td><td>H</td><td>53.7</td><td><strong>51.1</strong></td><td>56.3</td><td>57.3</td>
                  </tr>
                  <tr>
                    <td>-</td><td>-</td><td>âœ“</td><td>H</td><td>50.7</td><td>47.4</td><td>53.9</td><td>48.4</td>
                  </tr>
                  <tr>
                    <td>-</td><td>-</td><td>-</td><td>H</td><td>48.0</td><td>45.1</td><td>50.8</td><td>47.6</td>
                  </tr>
                  <tr>
                    <td>âœ“</td><td>âœ“</td><td>âœ“</td><td>S</td><td>50.1</td><td>46.7</td><td>53.5</td><td>60.2</td>
                  </tr>
                </tbody>
              </table>
              <p><small>Inv. = inversion, E-SW = extra stop words, SW = stop word filtering, SAM2 H/S = huge/small model, PA = point accuracy</small></p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Video Referral Segmentation -->

<!-- Societal Impact abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Societal Impact</h2>
        <div class="content has-text-justified">
          <p>
            Our RIFF and iRIFF methods provide powerful tools for extracting semantic features from rectified flow models, enabling advances in computer vision tasks such as semantic correspondence and referral segmentation. These capabilities have the potential to significantly enhance various applications including medical image analysis, robotics, autonomous systems, and assistive technologies for people with visual impairments.
            <br><br>
            By providing training-free, zero-shot methods that work across different domains (images and videos), our approach democratizes access to state-of-the-art semantic understanding capabilities. This is particularly valuable for researchers and practitioners who may not have access to large computational resources or extensive labeled datasets typically required for fine-tuning specialized models.
            <br><br>
            However, as with any advancement in computer vision and AI, there are potential ethical considerations. Improved semantic understanding capabilities could be misused for surveillance or privacy violation purposes. We emphasize the importance of deploying these technologies responsibly, with appropriate safeguards and consideration for privacy rights. We encourage the research community to continue developing ethical guidelines for the deployment of semantic feature extraction technologies and to consider the broader societal implications of these advancements.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Societal Impact abstract -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code></code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <!-- Lightbox for image zoom -->
  <div id="lightbox" class="lightbox" onclick="closeLightbox()">
    <span class="lightbox-close" onclick="closeLightbox()">&times;</span>
    <div class="lightbox-content">
      <img id="lightbox-img" src="" alt="">
    </div>
  </div>

  <script>
    function openLightbox(imgSrc, imgAlt) {
      const lightbox = document.getElementById('lightbox');
      const lightboxImg = document.getElementById('lightbox-img');
      lightboxImg.src = imgSrc;
      lightboxImg.alt = imgAlt;
      lightbox.style.display = 'block';
    }

    function closeLightbox() {
      document.getElementById('lightbox').style.display = 'none';
    }

    // Close lightbox with Escape key
    document.addEventListener('keydown', function(event) {
      if (event.key === 'Escape') {
        closeLightbox();
      }
    });

    // Add click handlers to all figure images
    document.addEventListener('DOMContentLoaded', function() {
      const figureImages = document.querySelectorAll('.figure-container img');
      figureImages.forEach(function(img) {
        img.addEventListener('click', function() {
          openLightbox(this.src, this.alt);
        });
      });
    });
  </script>

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
